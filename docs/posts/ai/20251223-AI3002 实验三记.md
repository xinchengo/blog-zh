---
date: 2025-12-23
---

# 支持向量机，决策树与集成学习

[人工智能与机器学习基础](https://github.com/ChangshuoShen/USTC-AI3002-25fall)的作业三和实验三下周三就要截止了，还是有必要简略地了解一下 SVM，决策树和集成学习是什么。这篇文章记录我完成作业时的简略笔记。

!!! info "关于该文章的书写进度"

    这篇文章仍在撰写中，将在完成作业时撰写完成。

<!-- more -->

## 支持向量机

### 普通 SVM

#### 目标函数

下面我们证明对于线性可分数据集，（硬间隔）SVM 的目标是：

$$
\min_{w,b}\frac{1}{2}\|w\|^2\quad \text{s.t.}\quad y_i(w^Tx_i+b)\ge 1
$$

回想一下，由于 SVM 的目标是在线性分隔数据的同时最大化最小间隔；根据几何学的知识，点 $x$ 到超平面 $f_{w,b}(x) = w^T x + b = 0$ 的距离为 $\frac{|f_{w,b}(x)|}{\|w\|}$；因此所有样本的最小间隔可以表示为：

$$
\begin{aligned}
\gamma &= \min_{i}\frac{|f_{w,b}(x_i)|}{\|w\|} \\
&= \min_i\frac{y_if_{w,b}(x_i)}{\|w\|}
\end{aligned}
$$

该等号成立是因为 SVM 中我们只考虑分类正确的样本，而标签的取值被定义为 $y_i \in \{+1, -1\}$。这样，我们可以得到 SVM 的目标函数：

$$
\max_{w,b}\gamma = \max_{w,b}\min_i \frac{y_i f_{w,b}(x_i)}{\|w\|} = \max_{w,b}\min_i \frac{y_i (w^Tx_i+b)}{\|w\|}
$$

为了简化这个目标函数，需要利用 $w,b$ 具有的一个性质——**伸缩不变性**，即，对于缩放操作 $w^\ast = cw,~b^\ast = bw$，$\lambda$ 不变，因此我们可以对 $w,b$ 增加正则化条件：

$$
\begin{gather}
y_i(w^Tx_i+b)\ge 1,\quad\exists i~~\text{s.t.}~\text{``}=\text{''} \\
\implies \gamma = \frac{1}{\|w\|} \\
\implies \arg\max_{w,b}\gamma = \arg\min_{w,b}\frac{1}{2}\|w\|^2
\end{gather}
$$

这样我们就完成了硬间隔 SVM 的目标函数推导。

### 软间隔 SVM

#### 目标函数

我们引入松弛因子，将正则化条件变化为：

$$
y_i(w^Tx_i + b) \ge 1 - \zeta_i
$$

在这个时候，最小化 $\|w\|^2$ 并不等价于最大化 $\lambda$，但是，我们仍然模仿硬间隔 SVM 的形式，写出软间隔 SVM 的目标函数：

$$
\min_{w,b,\zeta}\frac{1}{2}\|w\|^2,\quad \sum_{i}\zeta_i\le \epsilon
$$

使用 Lagrange 乘子法，这个问题也可以转化为：

$$
\begin{gather}
\min_{w,b,\zeta}\frac{1}{2}\|w\|^2+C\sum_{i=1}^n\zeta_i^2 \\
\text{with}\quad y_i(w^Tx_i+b)\ge 1 - \zeta_i,~\zeta_i \ge 0
\end{gather}
$$

#### 最小化范数与 VC 维理论

不管是在传统 SVM 中还是软间隔 SVM 中我们都在最小化一个 $\frac{1}{2}\|w\|^2$，这其实对应着一个原则——使用尽可能小、但仍包含正确/良好解的假设空间。这个原则叫做**结构化风险最小化**（structural risk minimization）。

这个原则其实来源于 VC 维理论（Vapnik-Chervonenkis dimension theory），由于：

$$
\text{VC dimension} \le \frac{R^2}{\gamma^2}\quad\text{with}\quad\gamma \le \frac{1}{\|w\|^2}
$$

我们在最小化 $\frac{1}{2}\|w\|^2$ 的同时，也在最小化 $w$ 的 VC 维，使模型尽可能地简单。

==TODO==：软间隔 SVM 原理的深入分析

#### Primal-Dual

由 Lagrange 乘子法：

$$
\begin{gather}
\min_{w}f(w)\\
g_{i} \le 0,~\forall 1\le i\le k\\
h_{i} = 0,~\forall 1\le i\le l\\
\implies \mathcal L(w,\boldsymbol\alpha,\boldsymbol\beta) = f(w)+\sum_{i}\alpha_i g_i(w) + \sum_{j}\beta_j h_j(w)
\end{gather}
$$
